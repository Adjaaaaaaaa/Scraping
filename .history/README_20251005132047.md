# Book Scraper & API

Projet réalisé dans le cadre de la formation **Développeur IA chez Simplon**.  
Ce projet scrape des livres depuis [Books to Scrape](http://books.toscrape.com/) et expose les données via une **API RESTful FastAPI**.  
Il inclut également la gestion de l’historique des modifications des livres.

---

## 1. Structure du projet

```
Scraping/
├─ books_api/
│ ├─ __init__.py
│ ├─ main.py # Point d’entrée FastAPI
│ ├─ presentation/
│ │ ├─ __init__.py
│ │ ├─ routes.py # Routes de l’API
│ │ └─ schemas.py # Schémas Pydantic
│ ├─ data/
│ │ ├─ __init__.py
│ │ ├─ database.py           # Connexion à la DB (vers books.db à la racine)
│ │ ├─ tables.py             # Mapping SQLAlchemy
│ │ └─ repositories.py       # Logique de récupération / filtres
│ └─ domain/
│   └─ models.py             # Modèles métier 
    └─__init__.py
│
├─ books_scraper/
│ ├─ books_scraper/          # Package Scrapy principal
│ │ ├─ __init__.py
│ │ ├─ items.py              # Définition des items
│ │ ├─ models.py             # Définition des tables Book et BookHistory
│ │ ├─ pipelines.py          # Nettoyage et sauvegarde dans DB
│ │ ├─ settings.py           # Paramètres Scrapy
│ │ ├─ database.py           # Connexion unique à la DB (racine)
│ │ └─ spiders/
│ │     ├─ __init__.py
│ │     └─ books.py          # Spider principal
│ ├─ scrapy.cfg
│
├─ books.db                  # Base SQLite unique
├─ run_daily.py              # Scheduler pour exécution automatique du spider
├─ requirements.txt
└─ README.md
```

---

## 2. Dépendances

- Python >= 3.10 recommandé
- Librairies :

```text
fastapi
uvicorn[standard]
sqlalchemy
pydantic
scrapy
schedule
```

## 3. Installation

### 1. Cloner le projet
```bash
git clone https://github.com/ton-utilisateur/scraping.git
cd scraping
```
### 2. Créer et activer l’environnement virtuel :

#### Windows (PowerShell)
```bash
python -m venv venv
.env\Scripts\Activate.ps1
```
#### macOS / Linux
```bash
python3 -m venv venv
source venv/bin/activate
```
### 3. Installer les dépendances 
```bash
pip install -r requirements.txt
```
---

## 4. Base de données

**SQLite** par défaut : `books.db`  

### Tables principales

**`books`** : informations sur les livres  

| Champ             | Type      |
|------------------|-----------|
| id               | Integer   |
| title            | String    |
| category         | String    |
| price            | Float     |
| availability     | String    |
| rating           | Integer   |
| url              | String    |
| copies_available | Integer   |
| upc              | String    |
| price_excl_tax   | Float     |
| price_incl_tax   | Float     |
| tax              | Float     |
| num_reviews      | Integer   |

**`book_history`** : historique des modifications  

| Champ        | Type      |
|-------------|-----------|
| id          | Integer   |
| book_upc    | String    |
| action      | String    | # added / updated / deleted
| old_data    | JSON      |
| new_data    | JSON      |
| change_date | DateTime  |

---

## 5. Scraping

### a) Lancer le spider manuellement

```bash
cd books_scraper
scrapy crawl books
```

- Scrape les livres depuis [Books to Scrape](http://books.toscrape.com/)  
- Nettoyage via `CleanBooksPipeline`  
- Sauvegarde et historique via `SaveBooksPipeline`

### b) Exécution automatique quotidienne

```bash
python run_daily.py
```

- Exécute le spider tous les jours à 02h00  
- Enregistre les changements dans `book_history`

---

## 6. API FastAPI

### a) Lancer le serveur

```bash
cd books_api
uvicorn books_api.main:app --reload
```

- Swagger UI : [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)  
- ReDoc : [http://127.0.0.1:8000/redoc](http://127.0.0.1:8000/redoc)

### b) Endpoints principaux

| Endpoint                             | Méthode | Description |
|--------------------------------------|---------|------------|
| `/books`                              | GET     | Liste tous les livres avec pagination (`skip`, `limit`) |
| `/books/upc/{upc}`                    | GET     | Récupère un livre par son UPC |
| `/books/price`                        | GET     | Filtre par prix (`min_price`, `max_price`) |
| `/books/category/{category}`          | GET     | Filtre par catégorie |
| `/books/availability/{availability}`  | GET     | Filtre par disponibilité (`In stock` / `Out of stock`) |
| `/books/search`                       | GET     | Recherche combinée : `category`, `min_price`, `max_price`, `availability`, `rating`, `skip`, `limit` |
| `/books/price-history/{upc}`          | GET     | Récupère l’**historique des variations de prix** d’un livre
---

### c) Exemple `/books/search`

```http
GET /books/search?category=Fiction&min_price=10&max_price=50&availability=In%20stock&rating=5&skip=0&limit=10
```

- Renvoie jusqu’à 10 livres de catégorie "Fiction", en stock, prix entre 10 et 50, et rating = 5

### d) Exemple `/books/price-history/{upc}`
Cet endpoint permet de visualiser l’évolution du prix d’un livre dans le temps.
Les variations sont enregistrées à chaque scraping quotidien dans la table book_history.

Exemple de réponse :
```
[
  {
    "upc": "a221fcb8dca0",
    "old_price": 12.99,
    "new_price": 13.76,
    "date": "2025-10-05T10:00:00"
  },
  {
    "upc": "a221fcb8dca0",
    "old_price": 13.76,
    "new_price": 13.50,
    "date": "2025-10-06T10:00:00"
  }
]
```
---

## 7. Interface API

L’API est documentée automatiquement grâce à **FastAPI**.  

### Swagger UI
Accessible via : [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

![Swagger UI](image.png)



---

## 8. Bonnes pratiques

1. Toujours lancer le scraper **avant d’interroger l’API** pour avoir des données récentes 
2. Utiliser `/books/search` pour combiner plusieurs filtres  
3. Pour filtrer par UPC, utiliser `/books/upc/{upc}`  
4. Vérifier la base avec SQLite si l’API retourne une liste vide  
